# Telemetry

## Intro

The goal of this Livebook is to provide an introduction to working with the [telemetry](https://github.com/beam-telemetry/telemetry) package in Elixir. We will cover the basics of creating and consuming telemetry events as well as how to integrate with something like [Phoenix LiveDashboard](https://github.com/phoenixframework/phoenix_live_dashboard).

## What is Telemetry

From the [documentation](https://hexdocs.pm/telemetry/readme.html):

> Telemetry is a lightweight library for dynamic dispatching of events, 
> with a focus on metrics and instrumentation.

With `telemetry`, you can generate events and attach data to those events. 
While this event and data combination could be anything, it's mostly commony used for,
well, telemetry data; data about your running system. An example of an event could be
a page request and for data, we may want to attach how how long the request took.

We cam think of this event, data pairing as a tuple of the event name paired
with a map of related data:

<!-- livebook:{"force_markdown":true} -->

```elixir
{:page_request, %{ duration: "250 ms"}}
```

<!-- livebook:{"break_markdown":true} -->

While the `telemetry` library works similarly to other event generation libraries 
it is not tied to a specific event or telemetry format. Instead, the focus of the 
`telemetry` library is to be a level _below_ these format-specific tools.

Rather than
being tied to a data format or service, `telemetry` aims to provide a BEAM-wide mechanism for 
creating (and consuming) events. If needed, these events can be consumed and converted to 
be compliant with the data format for your tool or service of choice. Existing solutions include 
[`telemetry_metrics_statsd`](https://github.com/beam-telemetry/telemetry_metrics_statsd) and 
[`telemetry_metrics_prometheus`](https://github.com/beam-telemetry/telemetry_metrics_prometheus).

Choosing to be the underlying event generation layer allows library authors to all use `telemetry`
without having to worry about their users expecting a differnt event format. Instead, library 
consumers can choose to listen for any events they want to track and, if necessary, convert the 
events into a format that works with their tool of choice. This choice has resulted in adoption
across many popular libraries and package such as 
[Ecto](https://hexdocs.pm/ecto/Ecto.Repo.html#module-telemetry-events),
[Oban](https://hexdocs.pm/oban/Oban.Telemetry.html),
[Phoenix](https://hexdocs.pm/phoenix/telemetry.html#phoenix-metrics),
and [many more](https://hex.pm/packages?search=depends%3Ahexpm%3Atelemetry).

## Setup

With introductions out of the way, let's move on to some examples.

Because `Mix.Install` can only be called once during a script run, we will be installing
all of our required libraries up front.

To get started, we will install the [`telemetry`](https://hex.pm/packages/telemetry) package.
We will also install the 
[`telemetry_metrics`](https://hex.pm/packages/telemetry_metrics)
package which provides functions for aggregating `telemetry` events into different 
metrics (e.g., counter, distribution).
Finally, what would a Livebook demo be without some graphs?! For this, 
we install `vega_lite` and `kino`.

```elixir
Mix.install([:telemetry, :telemetry_metrics, :vega_lite, :kino])
```

## Producing and Consuming Event

There are two main aspects to `telemetry` â€“ producing and consuming events.

To produce an event, we can use 
[`:telemetry.execute/3`](https://hexdocs.pm/telemetry/telemetry.html#execute/3) which takes

* The `Event Name`, which is made up of a list of atoms (e.g., `[:my, :cool, :event]`).
* The data, or `Measurements` which are provided via a map (e.g., `%{ duration: 100 }`)
* And, optionally, a map of `Metadata` which can be used to provide tags or any other useful 
  context. For Phoenix, this could be [the entire `conn` struct](https://hexdocs.pm/phoenix/telemetry.html#a-phoenix-example)
  associatd with the given event.

In the case of telemetry, if an event if create and no one is there to listen, it doesn't 
really _do_ anything.

```elixir
:telemetry.execute(
  [:falling, :tree],
  %{fall_time: 2.72},
  %{age: 289, type: "oak"}
)
```

Evaluating our sample call to `execute` simply returns `:ok`. Our event was emitted, but
we did not have any listeners around to receieve the event and do anything with it.

To consume an event, we turn to
[`:telemetry.attach/4`](https://hexdocs.pm/telemetry/telemetry.html#attach/4). The `attach`
function allows us to attach a function handler to a given event. It's arguments are:

* A `HandlerId` which is expected to be some for of unique identifier. Having a
  unique `HandlerId` allows us to later [`detach`](https://hexdocs.pm/telemetry/telemetry.html#detach/1)
  and event if needed. Behind the scenes, `telemetry` uses `ETS` to store and track our
  event handlers.

* The `EventName` we want to consume and take action on. Just like the `EventName` used
  when creating an event, this should also be a list of atoms.

* Our `HandlerFunction`, a four-arity function that is given the event we are listening for. 
  The function receives the `event` in four pieces:

  * `EventName`
  * `EventMeasurements`
  * `EventMetadata`
  * `Config`
    Most of these should be self-explanatory based on the arguments given to `execute/4`. The 
    only argument that doesn't come from `execute` is `Config`. The `Config` is also the
    fourth argument to `attach/4` â€“ when attaching to an event, we can optionally pass 
    in configuration that will be passed into our event handler.

* As mentioned above, our final argument is any `Config` that we may want to pass into
  our `HandlerFunction`.

## Listening for an event

Rather than having our events go into the void, let's create an event handler
and and have it listen for the `[:falling, :tree]` event type we emitted before.
Note: because we already emitted the previous `[:falling, :tree]` event we will **not**
be able to get access to it with our new event handler. Telemeetry events are ephemeral, 
so it is important register your event listeners before you start emitting the events you
are listening for.

For our first `HandlerFunction`, we  will simply log out the arguments we receive.

```elixir
:telemetry.attach(
  # unique handler id
  "my-first-handler",
  # event name
  [:falling, :tree],
  # event handler function
  fn event, measurements, metadata, config ->
    IO.puts("My first handler recieved a new event!\n")
    IO.inspect(event, label: "EventName")
    IO.inspect(measurements, label: "EventMeasurement")
    IO.inspect(metadata, label: "EventMetadata")
    IO.inspect(config, label: "config")
    IO.puts("\n-------------------------------------\n")
  end,
  # config to pass to our event handler
  my: :config
)
```

If you are following along in LiveBook and you executed the above cell for the first time,
we should see the result of `:ok`. If you end up re-evaluating the cell, you may an 
error tuple response, `{:error, :already_exists}`. As we mentioned above, our `HandlerID`
must be unique; re-evaluating the cell attempted to re-register our same ID. If this is the
case, no need to worry â€“ our handler will already be registered. If you want to see the `:ok`
response, you will need to reconnect to the LiveBook runtime.

Regardless of whether this was the firs time registering the event handle or not, you 
will see a warning about our `HandlerFunction` being a local function. The warning
helpfully explains that using an anonymous function can have performance reasons. 
For now, we do not need to worry about this, but we will address this in a future example.

Before we emit any events, let's confirm that we properly attached our handler. As noted before,
`telemetry` uses `ETS` tables to track event handlers. We can look up `telemetry`'s tracking
table and make sure our `my-first-handler` is properly registered.

```elixir
:ets.whereis(:telemetry_handler_table)
```

If you evaluate the cell above, the result should be a single row that contains information
about our event handler! ðŸ¥³

Our handler seems to be registered, but does it actually handle events when we emit them? 
Let's give it a try.

We will emit an event similar to our previous `[:falling, :tree]` event, but with different
`Measurements` and `Metadata` to make it clear this is a new event.

```elixir
:telemetry.execute(
  [:falling, :tree],
  %{fall_time: 1.49},
  %{age: 232, type: "red maple"}
)
```

This is the `telemetry` library in a nutshell. We can register
functions (via `attach`) that can handle events that we emit (via `execute`). 
That's it!

## Compiled Event Handlers

Before we move on let's make one small change to our `EventHandler` to avoid the warning
message we were receiving before. Instead of passing in an anonymous function, we will
pass in a Module.function capture.

```elixir
defmodule MorePerformantTelemetryHandler do
  # call out `handle_events` is not a special name; just a function 
  def handle_event(event, measurements, metadata, config) do
    IO.puts("MorePerformantTelemetryHandler recieved a new event!\n")
    IO.inspect(event, label: "EventName")
    IO.inspect(measurements, label: "EventMeasurement")
    IO.inspect(metadata, label: "EventMetadata")
    IO.inspect(config, label: "config")
    IO.puts("\n-------------------------------------\n")
  end
end

:telemetry.attach(
  # new HandlerID to be unique
  "more-performant-handler",
  # we will listen for the same event
  [:falling, :tree],
  # use a capture function for our module-defined function above
  &MorePerformantTelemetryHandler.handle_event/4,
  # we don't care about any config
  nil
)
```

After evaluating this cell, we should simply see the `:ok` return with no warning messages
logged.

Our new handler is listening for the same `[:falling, :tree]` events, so let's emit
yet another event.

```elixir
:telemetry.execute(
  [:falling, :tree],
  %{fall_time: 0.76},
  %{age: 812, type: "pine"}
)
```

Since we are using the same `EventName`, emitting our most recent event resulted in output 
from both `my-first-handler` **and** `more-performant-handler`.

While the identifier for our handler is expected to be unique, the event that is is listening 
for does not have to be. This means we can register multiple handlers for processing events.

## TODO

Show example of detaching anonymous version

## Listening for Multiple Events

In addition to having support for multiple handlers for a single event, you can also have
a single handler listen for multiple events via 
[`attach_many/4`](https://hexdocs.pm/telemetry/telemetry.html#attach_many/4).
The API is similar to `attach/4` covered above, but accepts a list of lists for the
`EventName` argument.

We will create a new event handler that processes events with our previously defined
`MorePerformantTelemetryHandler.handle_event` function, but will listen for new `EvenName`s â€“
`[:my_app, :request]` and `[:my_app, :query]`.

```elixir
:telemetry.attach_many(
  # unique handler id
  "my-app-event-handler",
  [
    [:my_app, :request],
    [:my_app, :query]
  ],
  &MorePerformantTelemetryHandler.handle_event/4,
  nil
)
```

We can now emit both types of events.

```elixir
:telemetry.execute(
  [:my_app, :request],
  %{duration: 250},
  %{route: "/", action: "GET"}
)

:telemetry.execute(
  [:my_app, :query],
  %{duration: 46},
  %{table: "users"}
)
```

With a small change in the arguments it handles `attach_many/4` provides us with a clean
interface for listening for and processing multiple events in the same manner.

## Storing Results

So far, our event handling has been pretty boring; all we have done is print out the event. 
Once the event is printed, it's gone. We may be able to scrape our logs to 
get access to the event data if we wanted, but maybe there are other options.

Instead of printing the event and moving on, let's instead **store** the information from the
event that we care about.

To do this, we can use an
[`Agent`](https://hexdocs.pm/elixir/Agent.html), an 
abstraction built out providing a simpe way to store state. Rather than having our handler
print out information about the event, we can store the revelant data in our `Agent` process.
Once in the `Agent`, we can retrieve our past event data as we need.

Please note that, in practice, an `Agent` isn't the best long-term solution for storing
our event data. Since it stores state
in a process, in-memory the data would not persiste through crashes and restarts. If you
want to keep your event data along long-term, you would like want to send it off to a 
service built around managing this sort of data.

<!-- livebook:{"break_markdown":true} -->

Our new "persisted" telemetry will be faux web request data.

* Our `EventName` will be `[:demo, :request, :complete]`.
* Our `EventMeasurements` will represent the `duration` a request took.
* Our `EventMetadata` will include information about the `route` requested 
  an the `timestamp` at which the request took place.

The resulting `telemetry.excute/3` call will look something like this:

<!-- livebook:{"force_markdown":true} -->

```elixir
:telemetry.execute(
  [:demo, :request, :complete],
  %{duration: 223}, 
  %{route: "/", timestamp: DateTime.utc_now()}
)
```

<!-- livebook:{"break_markdown":true} -->

Now that we have an idea of what our events will look like, we can define our
handler function.

The goal of this helper function is to take useful information from the event and store 
it in an `Agent` to be retreived later.

Our `event` will have useful data split across its measurements and metadata. To make 
it easier to consume our data later, our handler will combine everything into a single 
map entry before storing it in the `Agent`.

To get access to the `Agent` we want to use, we can take advantage of the final `config` 
argument provided by `:telemetry.attach/4`. At attachment-time, we will pass in the `PID`
of the `Agent` we want to use. Simply passing in the `PID` isn't a robust solution as it
would not survive the `Agent` being restarted. A more resilient soution would provide a
way to _look up_ the `Agent` process via something like 
[`Registry`](https://hexdocs.pm/elixir/Registry.html).

Let's take a look at our handle function below.

```elixir
defmodule StorageTelemetryHandler do
  def handle_event(
        event,
        %{duration: duration},
        %{route: route, timestamp: timestamp},
        agent: agent
      ) do
    new_event = %{event: event, duration: duration, route: route, timestamp: timestamp}

    Agent.get_and_update(agent, &{&1, [new_event | &1]})
  end
end
```

Before registering our handler, we will start up the `Agent` process that we will
be passing in via config.

```elixir
{:ok, agent} = Agent.start(fn -> [] end)
```

Now, we can register our handler of `[:demo, :request, :complete]` events with `:telemetry.attach/4`.

```elixir
:telemetry.attach(
  "storage-telemetry-handler",
  [:demo, :request, :complete],
  &StorageTelemetryHandler.handle_event/4,
  agent: agent
)
```

We can now fire off some `[:demo, :request, :complete]` events.

```elixir
# I could investigate making this a bit more dynamic and random
# - Range of Date for a longer stretch
# - Random duration and route

:telemetry.execute([:demo, :request, :complete], %{duration: 223}, %{
  route: "/",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 5212}, %{
  route: "/reports",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 412}, %{
  route: "/users",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 342}, %{
  route: "/",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 322}, %{
  route: "/",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 278}, %{
  route: "/users",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 302}, %{
  route: "/users",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 4123}, %{
  route: "/reports",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 4712}, %{
  route: "/reports",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 284}, %{
  route: "/",
  timestamp: DateTime.utc_now()
})

:telemetry.execute([:demo, :request, :complete], %{duration: 258}, %{
  route: "/users",
  timestamp: DateTime.utc_now()
})
```

Our `StorageTelemetryHandler` does not print anything out when it receives a message, so
creating our above events results in a simple `:ok` response. Instead of looking for output,
we can check our `Agent`'s state to make sure it has our telemetry.

```elixir
Agent.get(agent, & &1)
```

Our above output should reveal a list of our combined telemetry event maps.

<!-- livebook:{"break_markdown":true} -->

For many of us, it can be difficult to get an idea of larger data trends by looking at 
a large list of data.

To make this easier (and get the chance to have more fun with LiveBook), let's use the 
[VegaLite Elixir package](https://hexdocs.pm/vega_lite/VegaLite.html) to generate a 
graph of our data over time.

<!-- livebook:{"break_markdown":true} -->

Before we can pass our `Agent`'s data into, we need to make a few small tweaks.

1. Our `timestamp` is currently a `DateTime` struct. `VegaLite` doesn't know how to handle
   this, so we will also convert to a format that plays better using 
   [`DateTime.to_iso8601`](https://hexdocs.pm/elixir/DateTime.html#to_iso8601/3).

2. When storing new data in our `Agent` we were prepending our list of state. This 
   results in our data being ordered reverse cronologically. Since telemetry 
   data is generally expected to be viewed in cronological order, we will reverse
   the order of our data.

```elixir
data =
  Agent.get(agent, & &1)
  |> Enum.map(fn %{timestamp: timestamp} = data ->
    %{data | timestamp: DateTime.to_iso8601(timestamp)}
  end)
  |> Enum.reverse()
```

We can now use the `VegaLite` package to generate our graph ðŸ“ˆ We will be graphing a route's 
duration over time. To do this we will use a multi-line chart, giving us one line per route.

* We use the [`data_from_values`](https://hexdocs.pm/vega_lite/VegaLite.html#data_from_values/3)
  function to read in our `data`. This function support working with with a list of maps and 
  will pick out the keys specify for a given field.
* We indicate we want a `line` graph with the 
  [`mark`](https://hexdocs.pm/vega_lite/VegaLite.html#mark/3) function.
* We choose `timestamp` and `duration` for our `x` and `y` axes respectively
* Using `encode_field` with the value of `:color`, we are able to to `VegaLite` to use
  create a multi-line graph different colors for reach `route` value.

```elixir
alias VegaLite, as: Vl

Vl.new(width: 400, height: 400)
|> Vl.data_from_values(data)
|> Vl.mark(:line)
|> Vl.encode_field(:x, "timestamp", type: :ordinal)
|> Vl.encode_field(:y, "duration", type: :quantitative)
|> Vl.encode_field(:color, "route", type: :nominal)
```

With that, we now have a visual representation of our telemetry data.

## Telemetry.Metrics

So far we have been focused on emitting and consuming specific event types. 
For example, our `StorageTelemetryHandler.handle_event` function pattern-matches on expected 
measurements (`duration`) and metadata (`route`, `timestamp`).

<!-- livebook:{"force_markdown":true} -->

```elixir
def handle_event(
  event,
  %{duration: duration},
  %{route: route, timestamp: timestamp},
  agent: agent
) 
```

This exact matching is helpeful for understanding how things work, but isn't
particularly flexible. If we 
wanted to use our `StorageTelemetryHandler` to store other telemetry events,
we would likely not match on the specific
`handle_event` clause that we currently have and need to define (a) new one(s).

This is where a library like 
[`Telemetry.Metrics`](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html)
comes into play. `Telemetry.Metrics` adds standardization for extracting metrics from 
telemetry events.

Up until this point, we have been working with events and measurements.
We are now introducing the term metrics. In some cases, these may
feel like they could be used interchangeably. Before we move on, let's make understand
how they differ.

* `events` are what we create/emit with the `telemetry` library.
* `events` have `measurements` (and `metadata`).
* `measurements` are data included in our event. Defining a word with the 
  word itself, `measurements`, _tend_ to include data related to some measurment for the
  event (e.g., `duration` of an event for a request, `size` for an event related to system
  memeory usage).

To define `metrics`, we will turn to the `Telemetry.Metrics` documentation:

> Metrics are aggregations of Telemetry events with specific name, 
> providing a view of the system's behaviour over time.

`metrics` are an aggregation of `events`, and, generally, of a particular `measurement` of
the `events`.

The `Telemetry.Metrics` library provides an API for describing the metrics you want to extract
from your events (i.e., how you want to aggregate your measurements).
The API provides five different aggregation types:

1. [`counter`](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#counter/2) â€“
   count the number of events. Counters increase by one for reach event, regardless of 
   value of the event's measurement.
2. [`distribution`](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#distribution/2) â€“
   groups and buckets the data to create a histogram.
3. [`last_value`](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#last_value/2) â€“
   always return the value of the measurement from the most recent event.
4. [`sum`](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#sum/2) â€“
   tracks the running total of the specified measurement across events.
5. [`summary`](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#summary/2) â€“
   provides multiple statistics (e.g., `min`, `max`, `mean`),

One important thing to note is that `Telemetry.Metrics` does **not actually
do any of the aggregation**. From the projct's 
[rationale](https://hexdocs.pm/telemetry_metrics/rationale.html#content):

> unlike most of the libraries available on the BEAM, it doesn't aggregate metrics itself, it merely defines what users should expect when using the reporters.

Instead, the library 
[provides `Struct`s](https://github.com/beam-telemetry/telemetry_metrics/blob/8b8ce6a467dd87b6f9c1f4b5e45e93cb70f41788/mix.exs#L58-L64)
that can be relied upon when implementing a 
[reporter](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#module-reporters).
Again, from the project's rationale:

> If Telemetry.Metrics would aggregate metrics, the way those aggregations work would be imposed on the system where the metrics are published to. For example, counters in StatsD are reset on every flush and can be decremented, whereas counters in Prometheus are monotonically increasing. Telemetry.Metrics doesn't focus on those details - instead, it describes what the end user, operator, expects to see when using the metric of particular type.

`Telemetry.Metrics` provides the API and structure to pass metrics along to existing systems 
which are purpose-built for metric collection and aggregation. Not only does this design
choice prevent the need to re-implement highly performant aggregation logic, but it also
allows you to avoid performance issues by keeping this aggregation logic out of your 
application.

## Telemetry.Metrics - ConsoleReporter

Let's take a look at a simple 
[reporter](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#module-reporters)
that ships with `Telemetry.Metrics` itself, the
[`ConsoleReporter`](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.ConsoleReporter.html#content)
This reporter should feel similar to the first event handler functions we wrote; like those
functions, the `ConsoleReporter` simply logs information about the even that was emitted.

<!-- livebook:{"break_markdown":true} -->

While not a requirement, reporters are often implmented using a process-based solution.
This is the case for the `ConsoleReporter`.

We will start up a `ConsoleReporter` process and tell it that we want to collect two metrics:

1. The `last_value` for the event `[:demo, :telemetry_metrics, :time, :duration]`
2. A running `counter` for the event `[:demo, :telemetry_metrics, :count, :demo_count]`

```elixir
metrics = [
  Telemetry.Metrics.last_value([:demo, :telemetry_metrics, :time, :duration]),
  Telemetry.Metrics.counter([:demo, :telemetry_metrics, :count, :demo_count])
]

Supervisor.start_link([{Telemetry.Metrics.ConsoleReporter, metrics: metrics}],
  strategy: :one_for_one
)
```

Now, we can use the `telemetry` library to emit both types of events.

As a reminder, the `Telemetry.Metrics` library does not handle the metric aggregation itself. 
This is off-loaded to the service metrics are being sent to. The `ConsoleReporter` is 
implemented to simply log out the event it received and does not do any aggregation. This 
means that we will not get an actual incrementing `counter`. Instead, both values will 
behave like a `last_value` and give us information about hte most revent event.

```elixir
:telemetry.execute(
  [:demo, :telemetry_metrics, :count],
  %{demo_count: 2},
  %{}
)

:telemetry.execute(
  [:demo, :telemetry_metrics, :time],
  %{duration: 123.456},
  %{location: :third_example}
)
```

After our events are emitted, we can see log messages from the `ConsoleReporter` 
giving us information about our event!

This is a fantastic tool for debugging your `telemetry` setup. The best part is, 
using the same `telemetry` code, the metrics that you are collecting and debugging
with your console reporters can be configured to be consume by other reporters that can
do much more than logging. The only change you need to make is the reporter module you are using. 
Since our example had the `metrics` we want to collect in a variable, we could even pass 
the same list of metrics to the `ConsoleReporter` or our production-grade reporter based 
on our environment.

<!-- livebook:{"force_markdown":true} -->

```elixir
metrics = [
  Telemetry.Metrics.last_value([:demo, :telemetry_metrics, :time, :duration]),
  Telemetry.Metrics.counter([:demo, :telemetry_metrics, :count, :demo_count])
]

Supervisor.start_link([{Telemetry.Metrics.ConsoleReporter, metrics: metrics}],
  strategy: :one_for_one
)

Supervisor.start_link([{SomePackage.MyCoolReporter, metrics: metrics}],
  strategy: :one_for_one
)
```

## Telemetry.Metrics - Building a Reporter

Now that we've collected our first set of metrics using the `ConsoleReporter`,
let's get a better understanding of how reporters work and build our (very simple) one. 
To show the flexibility provided by building ontop of `Telemetry.Metrics` and not writing
our own custom handlers, we will re-visit using ETS for handlng event. However, this time,
instead of storing every telemetry event, we will be storing the _metrics_.

[The docs](https://hexdocs.pm/telemetry_metrics/writing_reporters.html#content) on how
to write a reporter do a great job of giving you the basics for wiring things up and does
a lot of our work for us. Our example will do the bare minimum to handle a few metrics types 
and does not fully follow the specification (e.g., we will not have a `keep?` check). 
If you want to build a real reporter, please take the time to read through the guide in the
docs to ensure you implement something to spec.

<!-- livebook:{"break_markdown":true} -->

As mention above, it is a common practice to wrap reporters in a process.
Therefore, our implementation will have two primary parts:

1. A process (via a `GenServer`). Our process will attach (using `:telemetry.attach` like we 
   have in previous examples) to the events we want metrics
   for. Our `init` callback and also be used to handle any other start-up work that we
   may need; in our case, this will include creating our ETS table.
2. A module that has the logic for actually processing the events.

<!-- livebook:{"break_markdown":true} -->

### Reporter Process

<!-- livebook:{"break_markdown":true} -->

For our reporter process, we will be using a `GenServer`. Our process will expect to be
passed in a list of `metrics` to handle when starting up. We expect to be given a list
`metrics` in the same format we saw above:

<!-- livebook:{"force_markdown":true} -->

```elixir
metrics = [
  Telemetry.Metrics.last_value([:demo, :telemetry_metrics, :time, :duration]),
  Telemetry.Metrics.counter([:demo, :telemetry_metrics, :count, :demo_count])
]
```

The `Telemetry.Metrics` functions return a struct of the specified metric type. As an example,
`Telemetry.Metrics.last_value` will return the struct `%Telemetry.Metrics.LastValue`. We
use the struct type to filter the list of metrics for those that we support (which, for our 
example, is a subset of the available options).

We will also use our `init` callback for creating the ETS table.

```elixir
defmodule ETSReporter.Server do
  use GenServer

  require Logger

  def start_link(options \\ []) do
    GenServer.start_link(__MODULE__, options)
  end

  def get_metrics(server) do
    GenServer.call(server, :get_metrics)
  end

  @supported_metrics [Telemetry.Metrics.Counter, Telemetry.Metrics.LastValue]
  def init(options) do
    # Fetch our list of metrics
    # While we would ideally support all metrics types, we are only implementing
    # a few simple examples for demo purposes. The docs suggest
    # fitlering and warning if there are metrics types we do not support. 
    # This also allows us to show what comes in from our `metrics` list.
    metrics =
      options
      |> Keyword.fetch!(:metrics)
      |> Enum.filter(fn
        %metric_type{} when metric_type in @supported_metrics ->
          true

        %metric_type{} ->
          Logger.warn("We do not current support #{metric_type} metrics")
          false
      end)

    # Create the ETS table we will use for storing our metric data
    ets = :ets.new(table_name(), [:set, :public])

    # Attach an event listener for the events associated with the
    # metrics we are collecting. The handler function is defined in
    # a module we will cover next.
    for {event, metrics} <- Enum.group_by(metrics, & &1.event_name) do
      id = {__MODULE__, event, self()}
      IO.inspect(event, label: "Event")
      IO.inspect(metrics, label: "metrics")

      :telemetry.attach(
        id,
        event,
        &ETSReporter.Handler.handle_event/4,
        metrics: metrics,
        ets: ets
      )
    end

    {:ok, %{metrics: metrics, ets: ets}}
  end

  def handle_call(:get_metrics, _, %{ets: ets} = state) do
    {:reply, ets, state}
  end

  defp table_name() do
    "#{__MODULE__}_#{System.os_time(:second)}" |> String.downcase() |> String.to_atom()
  end
end
```

### Handler Module

Let's look at our handler module. This module has a single public function, 
`handle_event`.

Note: while the function name `handle_event` may seem like a required name
(like `handle_call`, `handle_cast`), it is not. `handle_event` seems to be a common
choice for reporters (likely influenced by the `GenServer` callback functions), but is
not a formal `Behaviour` callback.

If you went through the section on building our own
`telemetry` event handler, this function should look familiar.As a reminder,
let's take a look at the `handle_event` function we defined 
for our `MorePerformantTelemetryHandler`:

<!-- livebook:{"force_markdown":true} -->

```elixir
defmodule MorePerformantTelemetryHandler do
  def handle_event(event, measurements, metadata, config)
end
```

Our reporter handler will take in the same arguments for its `handle_event` 
function.

```elixir
defmodule ETSReporter.Handler do
  def handle_event(event_name, measurements, _metadata, metrics: metrics, ets: ets) do
    for metric <- metrics do
      try do
        update_entry(ets, event_name, metric, measurements)
      rescue
        e ->
          Logger.error("Could not format metric #{inspect(metric)}")
          Logger.error(Exception.format(:error, e, __STACKTRACE__))
      end
    end
  end

  defp update_entry(ets, event_name, %Telemetry.Metrics.Counter{} = metric, _measurements) do
    :ets.update_counter(ets, key(event_name, metric), 1, {1, 0})
  end

  defp update_entry(ets, event_name, %Telemetry.Metrics.LastValue{} = metric, measurements) do
    value = Map.fetch!(measurements, metric.measurement)

    :ets.insert(ets, {key(event_name, metric), value})
  end

  defp key(event_name, %type{} = metric) do
    (event_name ++ [metric.measurement, type]) |> List.to_tuple()
  end
end
```

A difference you may notice in our handler's `handle_event` function is that one of our options
is the list of `metrics` that we are listening for. This list was passed in during the
`ETSReport.Server.init` callback.

`metrics` are aggregations of `events` and you can have multiple
aggregation types for a single event. For example,
we may to have the `count` and the `last_value` for a given event.
In order to support a single event handler for each event, our event handler function 
must iterate over the list of `metrics` we are collecting for the event that we are handling.

The handling is done in our `update_entry` functions. We leverage pattern matching to separate
out the implementations for each metrics type that we support. _This_ is where the logic
specific to our `Reporter` lives. Since our `Reporter` is using ETS to store the data, these
functions udpate our ETS table based on the type of metric. If we were a StatsD reporter, we 
would be sending the data to our StatsD collector.

## Using our Reporter

TODO - here we will show how to use our reporter. We will need to pull in code from our 
other LiveBook.

## Telemetry.Metrics - Reporter Libraries

In practice, this means you will often reach for a libary built ontop of `Telemetry.Metrics`, 
one that is built to work with your particular metrics collection technology stack.
Two popular example are
[`TelemetryMetricsStatsd`](https://hexdocs.pm/telemetry_metrics_statsd/TelemetryMetricsStatsd.html) 
and [`TelemetryMetricsPrometheus.Core`](https://hexdocs.pm/telemetry_metrics_prometheus_core/TelemetryMetricsPrometheus.Core.html),
both maintained by the [`beam-telemetry` org on GitHub](https://github.com/beam-telemetry).

Despite these services working differently (with StatsD you push metrics to a StatsD collector
while Promethesus will periodically pull data from your application endpoint), by building 
ontop of the `Telemetry.Metrics` abstraction you do not have to think about these differences 
when setting up your metric collection. With both options, you use the `telemetry` library 
to generate your events and a wrapper around the `Telemetry.Metrics` API to set up how you 
want to covert the events in to metrics.

To see just how similar set up is, let's take a look at the set up examples from both projects.

**TelemetryMetricsPrometheus.Core**

<!-- livebook:{"force_markdown":true} -->

```elixir
def start(_type, _args) do
  # List all child processes to be supervised
  children = [
    {TelemetryMetricsPrometheus.Core, [
      metrics: [
        counter("http.request.count"),
        sum("http.request.payload_size", unit: :byte),
        sum("websocket.connection.count", reporter_options: [prometheus_type: :gauge]),
        last_value("vm.memory.total", unit: :byte)
      ]
    ]}
  ]

  opts = [strategy: :one_for_one, name: ExampleApp.Supervisor]
  Supervisor.start_link(children, opts)
end
```

and **TelemetryMetricsStatsd**

<!-- livebook:{"force_markdown":true} -->

```elixir
import Telemetry.Metrics

TelemetryMetricsStatsd.start_link(
  metrics: [
    counter("http.request.count"),
    sum("http.request.payload_size"),
    last_value("vm.memory.total")
  ]
)
```

Most of the difference is from `TelemetryMetricsPrometheus.Core` using an example where they
leverage the supervision tree and `TelemetryMetricsStatsd` showing a more basic `start_link`.

The libraries are able to provide additional configuration to better suit their target system
(you may have noticed the `prometheus_type` argument in our Prometheus example), but the core
is the same. Metric libraries no longer need to implement every part of creating metrics (an API
for creating events, an API for which events to listen to and send off, etc.). 
The power comes from the abstractions defined by `Telemetry.Metrics`, specifically the 
ability to process the events from `Telemetry.Metrics` through the concept of [reporters](https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#module-reporters).
The documentation summaries this nicely:

> Reporters take metric definitions as an input, subscribe to relevant events and aggregate data when the events are emitted. Reporters may push metrics to StatsD, some time-series database, or exposing a HTTP endpoint for Prometheus to scrape. In a nutshell, `Telemetry.Metrics` defines only how metrics of particular type should behave and reporters provide the actual implementation for these aggregations.

When first encountering `Telemetry.Metrics`, it may be off-putting trying to understand what it 
actually _does_. Unlike many libraries you may pull into your application, `Telemetry.Metrics`
is more akin to a specification than directly providing you with metrics (though it can do that
too!).

### Next Steps

* include links to sample reporters that work with different platforms
* use the console reporter to different events and metrics for them
* explore creating a custom reporter

## LiveDashboard

* LiveDashboard leverages `Telemetry.Metrics`
* Phoenix apps already have this set up
* LiveBook is a Phoenix app
* Cannot add after, but we can send metrics that it's looking for

Event

* Phoenix uses Plug.Telemetry https://github.com/phoenixframework/phoenix/blob/41435470bc414b859497cd03a5b39e08da659368/installer/templates/phx_web/endpoint.ex#L39

<!-- livebook:{"break_markdown":true} -->

Current `LivebookWeb.Telemetry` module (https://github.com/livebook-dev/livebook/blob/main/lib/livebook_web/telemetry.ex)

<!-- livebook:{"force_markdown":true} -->

```elixir
  def metrics do
    [
      # Phoenix Metrics
      summary("phoenix.endpoint.stop.duration",
        unit: {:native, :millisecond}
      ),
      summary("phoenix.router_dispatch.stop.duration",
        tags: [:route],
        unit: {:native, :millisecond}
      ),

      # VM Metrics
      summary("vm.memory.total", unit: {:byte, :kilobyte}),
      summary("vm.total_run_queue_lengths.total"),
      summary("vm.total_run_queue_lengths.cpu"),
      summary("vm.total_run_queue_lengths.io")
    ]
  end
```

```elixir
Supervisor.start_link(
  [
    {Telemetry.Metrics.ConsoleReporter,
     metrics: [
       Telemetry.Metrics.summary("phoenix.endpoint.stop.duration",
         unit: {:native, :millisecond}
       )
     ]}
  ],
  strategy: :one_for_one
)
```

```elixir
:telemetry.execute(
  [:phoenix, :endpoint, :stop],
  %{duration: 8_897_129_000},
  %{}
)

:telemetry.execute(
  [:phoenix, :endpoint, :stop],
  %{duration: 8_212_129_000},
  %{}
)

:telemetry.execute(
  [:phoenix, :endpoint, :stop],
  %{duration: 9_897_129_000},
  %{}
)
```

When running Livebook in standalone mode, it spawns a new Node

```elixir
[livebook_node] = Node.list(:connected)
```

```elixir
[88_971_129, 74_873_619, 85_445_795]
|> Enum.each(fn duration ->
  :rpc.call(livebook_node, :telemetry, :execute, [
    [:phoenix, :endpoint, :stop],
    %{duration: duration},
    %{}
  ])

  Process.sleep(500)
end)
```

You should now see some metrics

## Ideas

* [ ] Show the ETS table that registers events?  https://github.com/beam-telemetry/telemetry/blob/v1.0.0/src/telemetry_handler_table.erl)
* [ ] Telemetry.Metrics `keep?` (https://hexdocs.pm/telemetry_metrics/Telemetry.Metrics.html#keep_fun/2), not overflowing system, using with different backends
* [ ] Staring a separte Phoenix server to get events? (https://gist.github.com/teamon/a964dd80bc5c6802e3a8548efdda9cff)
* [ ] Architecture of telemetry (the library, within your app)
* [ ] Why telemetry is valuable
* [ ] Using StatsD reporter, Prometheus
* [ ] optionally using reports; console reporters
* [ ] Why are `Telemetry.Metrics` reporters usually processes?Guesses:
  * start as a part of your application start up; can be restarted via supervisor
  * statsd version looks like it can handle issues with UDP and re-establish the connection, hold connection pool that reporters can ask for
